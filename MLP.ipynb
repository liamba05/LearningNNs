{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class Value():\n",
    "    \"\"\"\n",
    "    Value class that stores a single value, its own gradient, and its children for backpropagation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=()):\n",
    "        self.data = data\n",
    "        self.grad = 0.0 # Set gradient to 0 automatically. Should be calculated and set in backprop.\n",
    "        self._backward = lambda: None # Gradient calculation. Unique dependent on operation. Set as none by default. Will be set when opreation complete.\n",
    "        self._prev = set(_children) # Don't care for duplicate children\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"Value(data={self.data}, grad={self.grad})\")\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other) # Allows value + number to take place\n",
    "        out = Value(self.data + other.data, (self, other)) # Set children nodes and assign operation for backpropagation\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad # Due to addition, child gradients should be same as parent (added in case of multi of same node)\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward # Assign backprop lambda\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)) # Only supporting int/float exponentiation\n",
    "        out = Value(self.data**other, (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "            # Calculus power rule and chain rule to calculate gradient\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        return -self + other\n",
    "    \n",
    "    # Functions above are just QOL functions for when we try to do ops using ints/floats rather than values\n",
    "\n",
    "    def sigmoid(self): # Sigmoid activation function for non-linearity\n",
    "        out = Value(1 / (1 + math.exp(-self.data)), (self,))\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.data * (1 - out.data) * out.grad\n",
    "            # Sigmoid derivative calculated and written out\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t, (self, ))\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def ReLU(self):\n",
    "        x = np.maximum(self.data, 0)\n",
    "        out = Value(x, (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 if x > 0 else 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def exp(self): # e^x\n",
    "        x = self.data\n",
    "        out = Value(np.exp(x), (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad # e^x dvtv is e^x and we also incorporate chain rule\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def log(self):\n",
    "        x = self.data\n",
    "        out = Value(math.log(x), (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1/x) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self) # This topological list is children first. i.e. for each node, traverse all the way down our stored children\n",
    "        # Then once we reach a node with no children, add that node to our list. This gives us an ordered list of children first for each node we have.\n",
    "        self.grad = 1.0 # This should be called on the final output value which should have a gradient of 1\n",
    "        for node in reversed(topo): # For each node in reversed topology list (Meaning nodes descending starting at output node, exactly what you want in backprop)\n",
    "            node._backward() # Calculate given node's gradient and assign it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \"\"\"\n",
    "    Neuron class representing input/output calculations alongside specified activation function.\n",
    "    Takes in # of inputs in initialization (also represents the num weights)\n",
    "    \"\"\"\n",
    "    def __init__(self, numInputs, activation):\n",
    "        self.activation = activation\n",
    "        self.weights = [Value(np.random.uniform(-0.1, 0.1)) for _ in range(numInputs)]\n",
    "        self.bias = Value(np.random.uniform(-0.1,0.1))\n",
    "        # Initialize neuron with random weights and random bias\n",
    "\n",
    "    def __call__(self, x): # f(x) call\n",
    "        x = np.array(x) if not isinstance(x, np.ndarray) else x\n",
    "        pre_activate = sum((wi*xi for wi, xi in zip(self.weights, x)), self.bias) # Calculate output (weights * input). Note: is Value class\n",
    "\n",
    "        if self.activation == \"tanh\":\n",
    "            return pre_activate.tanh()\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return pre_activate.sigmoid()\n",
    "        else:\n",
    "            return pre_activate.ReLU()\n",
    "    \n",
    "    def getParams(self):\n",
    "        return self.weights + [self.bias] # Return all weights and biases (params) within singular neuron (Necessary for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A singular layer containing n amount of neurons\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, numInputs, numNeuronsInLayer, activation):\n",
    "        # Create desired # of neurons for the layer that take in n inputs.\n",
    "        # numNeuronsInLayer is the number of outputs.\n",
    "        self.neurons = [Neuron(numInputs, activation) for _ in range(numNeuronsInLayer)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Calculate outputs of each neuron in layer and return them\n",
    "        outputs = [neu(x) for neu in self.neurons]\n",
    "        return outputs[0] if len(outputs) == 1 else outputs\n",
    "    \n",
    "    def getParams(self):\n",
    "        # Return all weights and biases in all neurons in this layer \n",
    "        return [p for neuron in self.neurons for p in neuron.getParams()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumOptimizer:\n",
    "    \"\"\"\n",
    "    Basic momentum gradient descent optimizer class.\n",
    "    Can set beta to 0 for regulat gradient descent.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, alpha=0.1, beta=0.9):\n",
    "        self.params = params\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.velocities = [np.zeros_like(p.data) for p in params] if beta > 0 else None\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad = 0.0\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Updates each parameter using momentum-based gradient descent:\n",
    "           v = beta * v + (1 - beta) * grad\n",
    "           p = p - lr * v\n",
    "        \"\"\"\n",
    "        for idx, p in enumerate(self.params):\n",
    "            if self.beta > 0:  # Momentum update\n",
    "                previousV = self.velocities[idx]\n",
    "                v = previousV * self.beta + (1 - self.beta) * p.grad\n",
    "                self.velocities[idx] = v\n",
    "            else:  # Perform regular gradient descent if beta is 0 or less\n",
    "                v = p.grad\n",
    "\n",
    "            p.data -= v * self.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSPropOptimizer:\n",
    "\n",
    "    def __init__(self, params, alpha=0.0001, beta=0.9, eps=1e-8):\n",
    "        self.params = params\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "\n",
    "        self.cache = [0.0 for _ in params]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad = 0.0\n",
    "    \n",
    "    def step(self):\n",
    "        for i, p in enumerate(self.params):\n",
    "            # Update running average of squared gradients\n",
    "            self.cache[i] = self.beta * self.cache[i] + (1.0 - self.beta) * (p.grad ** 2)\n",
    "            \n",
    "            # Apply the RMSProp update\n",
    "            p.data -= self.alpha * p.grad / (np.sqrt(self.cache[i]) + self.eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AdamOptimizer:\n",
    "    \"\"\"\n",
    "    Implementation of the Adam Optimizer.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, alpha=1e-4, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.params = params\n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "\n",
    "\n",
    "        # Initialize first and second moment estimates to zero\n",
    "        self.m = [0.0 for _ in params]\n",
    "        self.v = [0.0 for _ in params]\n",
    "\n",
    "        # Time step (for bias correction)\n",
    "        self.t = 0\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad = 0.0\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single Adam update for each parameter:\n",
    "\n",
    "            t = t + 1\n",
    "            m_t = beta1 * m_{t-1} + (1 - beta1) * grad\n",
    "            v_t = beta2 * v_{t-1} + (1 - beta2) * (grad^2)\n",
    "\n",
    "            m_hat = m_t / (1 - beta1^t)\n",
    "            v_hat = v_t / (1 - beta2^t)\n",
    "\n",
    "            param = param - alpha * m_hat / (sqrt(v_hat) + eps)\n",
    "        \"\"\"\n",
    "        self.t += 1  # increment time step\n",
    "        for i, p in enumerate(self.params):\n",
    "            # 1) Update first moment (m) and second moment (v)\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * p.grad\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (p.grad ** 2)\n",
    "\n",
    "            # 2) Compute bias-corrected estimates\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # 3) Adam update\n",
    "            p.data -= self.alpha * m_hat / (np.sqrt(v_hat) + self.eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(xs, ys, batch_size=32):\n",
    "    # pick random indices\n",
    "    idxs = np.random.choice(len(xs), batch_size, replace=False)\n",
    "    x_batch = [xs[i] for i in idxs]\n",
    "    y_batch = [ys[i] for i in idxs]\n",
    "    return x_batch, y_batch\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    A multi-layered perceptron implementation that puts everything together and is trainable.\n",
    "    Takes in # of inputs and\n",
    "        numNeuronsPerLayer (list): Ordered list representing the amount of layers we want (length) and how many neurons we want in each layer (values)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, numInps, numNeuronsPerLayer, activation='tanh', loss_function='mse'):\n",
    "        sizes = [numInps] + numNeuronsPerLayer \n",
    "        self.layers = [Layer(sizes[i], sizes[i+1], activation) for i in range(len(numNeuronsPerLayer))] # \n",
    "        self.activation = activation\n",
    "        self.loss_function = loss_function\n",
    "        # We know that the # of inputs for the next layer is the same as the number of outputs for this layer. So we can use sizes[i] and sizes[i+1] to classify that\n",
    "\n",
    "    def compute_loss(self, ys, yhats):\n",
    "        if self.loss_function == \"mse\":\n",
    "            return sum((yhat - y)**2 for y, yhat in zip(ys, yhats))\n",
    "        elif self.loss_function == \"ce\": # binary cross_entropy\n",
    "            return -sum(y * yhat.log() + (1 - y) * (1 - yhat).log() for y, yhat in zip(ys, yhats))\n",
    "            \n",
    "\n",
    "    def __call__(self, x): # This is the forward pass of the MLP. x represents the user input to the neural network.\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            # Calculate vector of neuron outputs using the previous layer's outputs as inputs\n",
    "            # The output of the last layer is what is returned\n",
    "        return x\n",
    "    \n",
    "    def getParams(self): # Get params of all neurons in all layers in order of layers\n",
    "        return [p for layer in self.layers for p in layer.getParams()]\n",
    "    \n",
    "    def train(self, xs, ys, optimizer, max_iter=500):\n",
    "        if not isinstance(optimizer, MomentumOptimizer) or not isinstance(optimizer, RMSPropOptimizer) or not isinstance(optimizer, AdamOptimizer):\n",
    "            assert TypeError(\"Please use a pre-made optimizer\")\n",
    "        # Training loop\n",
    "        for epoch in range(max_iter):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_b, y_b = get_batch(xs, ys, batch_size=24)\n",
    "            \n",
    "            # Forward pass - make prediction\n",
    "            y_preds = [self(x) for x in x_b]  # y_preds is a list of Value objects\n",
    "            \n",
    "            loss = self.compute_loss(y_b, y_preds)\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss = {loss.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('/Users/liambouayad/Documents/Coding/Personal/Neural_Networks/micrograd/wine+quality/winequality-white.csv', sep=';')\n",
    "\n",
    "df[\"quality\"] = 1*(df[\"quality\"]>5)\n",
    "\n",
    "\n",
    "X = df.drop(columns=['quality'])\n",
    "y = df['quality']\n",
    "\n",
    "def scale_columns(dataframe, cols):\n",
    "    scaled_df = dataframe.copy()\n",
    "    for c in cols:\n",
    "        mean = scaled_df[c].mean()\n",
    "        std = scaled_df[c].std()\n",
    "        scaled_df[c] = (scaled_df[c] - mean) / std\n",
    "    return scaled_df\n",
    "\n",
    "X = scale_columns(X, X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(X_train.shape[1], [8, 1], activation='sigmoid', loss_function=\"ce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamOptimizer(mlp.getParams(), alpha=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = 16.53284003465768\n",
      "Epoch 50, Loss = 15.694259144907848\n",
      "Epoch 100, Loss = 15.216361372850153\n",
      "Epoch 150, Loss = 16.773477063925203\n",
      "Epoch 200, Loss = 16.07770029090445\n",
      "Epoch 250, Loss = 16.30729368217303\n",
      "Epoch 300, Loss = 15.571666173658041\n",
      "Epoch 350, Loss = 16.050378082677522\n",
      "Epoch 400, Loss = 16.546188070499376\n",
      "Epoch 450, Loss = 16.28323486266553\n",
      "Epoch 500, Loss = 15.472633305603075\n",
      "Epoch 550, Loss = 14.955410675013523\n",
      "Epoch 600, Loss = 14.890544957621579\n",
      "Epoch 650, Loss = 14.880944636240143\n",
      "Epoch 700, Loss = 14.515437109601903\n",
      "Epoch 750, Loss = 15.402785450814827\n",
      "Epoch 800, Loss = 15.664400010347766\n",
      "Epoch 850, Loss = 18.373239081322826\n",
      "Epoch 900, Loss = 16.860359792145406\n",
      "Epoch 950, Loss = 15.943738844325278\n",
      "Epoch 1000, Loss = 15.293420500197941\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Xt = X_train.values.tolist()\n",
    "yt = y_train.values.tolist()\n",
    "\n",
    "X_toTrain = []\n",
    "\n",
    "for x in Xt:\n",
    "    temp = []\n",
    "    for x_i in x:\n",
    "        temp.append(float(x_i))\n",
    "    X_toTrain.append(temp)\n",
    "\n",
    "y_toTrain = [float(y) for y in yt]\n",
    "\n",
    "mlp.train(X_toTrain[:100], y_toTrain[:100], optimizer, max_iter=1001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "quality\n",
      "1    0.665169\n",
      "0    0.334831\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Feature correlations with quality:\n",
      "quality                 1.000000\n",
      "alcohol                 0.383280\n",
      "pH                      0.083687\n",
      "sulphates               0.051858\n",
      "citric acid            -0.000700\n",
      "free sulfur dioxide    -0.001278\n",
      "fixed acidity          -0.089749\n",
      "residual sugar         -0.092756\n",
      "total sulfur dioxide   -0.170924\n",
      "chlorides              -0.183939\n",
      "volatile acidity       -0.225440\n",
      "density                -0.268696\n",
      "Name: quality, dtype: float64\n",
      "\n",
      "Feature statistics:\n",
      "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "count    4898.000000       4898.000000  4898.000000     4898.000000   \n",
      "mean        6.854788          0.278241     0.334192        6.391415   \n",
      "std         0.843868          0.100795     0.121020        5.072058   \n",
      "min         3.800000          0.080000     0.000000        0.600000   \n",
      "25%         6.300000          0.210000     0.270000        1.700000   \n",
      "50%         6.800000          0.260000     0.320000        5.200000   \n",
      "75%         7.300000          0.320000     0.390000        9.900000   \n",
      "max        14.200000          1.100000     1.660000       65.800000   \n",
      "\n",
      "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
      "count  4898.000000          4898.000000           4898.000000  4898.000000   \n",
      "mean      0.045772            35.308085            138.360657     0.994027   \n",
      "std       0.021848            17.007137             42.498065     0.002991   \n",
      "min       0.009000             2.000000              9.000000     0.987110   \n",
      "25%       0.036000            23.000000            108.000000     0.991723   \n",
      "50%       0.043000            34.000000            134.000000     0.993740   \n",
      "75%       0.050000            46.000000            167.000000     0.996100   \n",
      "max       0.346000           289.000000            440.000000     1.038980   \n",
      "\n",
      "                pH    sulphates      alcohol      quality  \n",
      "count  4898.000000  4898.000000  4898.000000  4898.000000  \n",
      "mean      3.188267     0.489847    10.514267     0.665169  \n",
      "std       0.151001     0.114126     1.230621     0.471979  \n",
      "min       2.720000     0.220000     8.000000     0.000000  \n",
      "25%       3.090000     0.410000     9.500000     0.000000  \n",
      "50%       3.180000     0.470000    10.400000     1.000000  \n",
      "75%       3.280000     0.550000    11.400000     1.000000  \n",
      "max       3.820000     1.080000    14.200000     1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Check class balance\n",
    "print(\"Class distribution:\")\n",
    "print(df['quality'].value_counts(normalize=True))\n",
    "\n",
    "# Check feature correlations\n",
    "correlations = df.corr()['quality'].sort_values(ascending=False)\n",
    "print(\"\\nFeature correlations with quality:\")\n",
    "print(correlations)\n",
    "\n",
    "# Check for extreme values\n",
    "print(\"\\nFeature statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.6633\n"
     ]
    }
   ],
   "source": [
    "# 3. Add validation metrics\n",
    "def calculate_accuracy(model, X, y):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x_i, y_i in zip(X, y):\n",
    "        pred = model(x_i).data > 0.5\n",
    "        correct += (pred == y_i)\n",
    "        total += 1\n",
    "    return correct/total\n",
    "\n",
    "# During training, every 50 epochs:\n",
    "train_acc = calculate_accuracy(mlp, X_toTrain, y_toTrain)\n",
    "print(f\"Training accuracy: {train_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSummarized TODO\\n\\nImplement dropout ? (Maybe)\\nVisualizations\\nTrain on real dataset and use scikit learn to document results (f1 score, confusion matrix)\\nSince wine set isnt working well, try MNIST\\n\\n\\nBuilt an extensible MLP library from scratch, implementing backpropagation, optimizers (SGD, Adam), and modular activation/loss functions.\\nApplied the model to practical tasks, including fraudulent transaction detection (99.2% F1 score) and image classification (97% on MNIST).\\nDeveloped tools for batch processing, saving/loading models, and training visualization, competing with Scikit-learn and PyTorch on small-scale datasets.\\n\\n'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summarized TODO\n",
    "\n",
    "Implement dropout ? (Maybe)\n",
    "Visualizations\n",
    "Train on real dataset and use scikit learn to document results (f1 score, confusion matrix)\n",
    "Since wine set isnt working well, try MNIST\n",
    "\n",
    "\n",
    "Built an extensible MLP library from scratch, implementing backpropagation, optimizers (SGD, Adam), and modular activation/loss functions.\n",
    "Applied the model to practical tasks, including fraudulent transaction detection (99.2% F1 score) and image classification (97% on MNIST).\n",
    "Developed tools for batch processing, saving/loading models, and training visualization, competing with Scikit-learn and PyTorch on small-scale datasets.\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
